{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Controller for Robotics\n",
    "\n",
    "This notebook demonstrates how to implement a policy gradient controller for robotic systems, as discussed in Chapter 9 of the Physical AI & Humanoid Robotics book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy and Value Networks\n",
    "\n",
    "First, let's define the neural network architectures for our policy gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network that maps states to action probabilities.\n",
    "    For continuous action spaces, we output the parameters of a distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output mean for each action dimension\n",
    "        self.action_mean = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Log standard deviation for each action dimension (learnable)\n",
    "        self.action_log_std = nn.Parameter(torch.zeros(action_size))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.fc_layers(state)\n",
    "        action_mean = torch.tanh(self.action_mean(features))  # Bound mean to [-1, 1]\n",
    "        action_std = torch.exp(self.action_log_std)\n",
    "        return action_mean, action_std\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Value network that estimates the value of a state.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, hidden_size=64):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Gradient Agent\n",
    "\n",
    "Now let's implement the policy gradient agent that will learn to control our robotic system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    \"\"\"\n",
    "    Policy Gradient Agent that learns to control a robotic system.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate=3e-4, gamma=0.99, entropy_coef=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # Initialize policy and value networks\n",
    "        self.policy = PolicyNetwork(state_size, action_size)\n",
    "        self.value = ValueNetwork(state_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Storage for episode data\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action based on the current policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state of the environment\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "            log_prob: Log probability of the selected action\n",
    "            value: Estimated value of the state\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Get action distribution parameters and state value\n",
    "        action_mean, action_std = self.policy(state_tensor)\n",
    "        value = self.value(state_tensor)\n",
    "        \n",
    "        # Create distribution and sample action\n",
    "        dist = torch.distributions.Normal(action_mean, action_std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        \n",
    "        return action.detach().cpu().numpy()[0], log_prob, value, entropy\n",
    "    \n",
    "    def store_transition(self, log_prob, value, entropy, reward):\n",
    "        \"\"\"\n",
    "        Store transition information for later update.\n",
    "        \n",
    "        Args:\n",
    "            log_prob: Log probability of the action taken\n",
    "            value: Estimated value of the state\n",
    "            entropy: Entropy of the action distribution\n",
    "            reward: Reward received\n",
    "        \"\"\"\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.entropies.append(entropy)\n",
    "        self.rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        Update the policy based on the collected episode data.\n",
    "        \"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return\n",
    "        \n",
    "        # Compute discounted returns\n",
    "        returns = deque(maxlen=len(self.rewards))\n",
    "        R = torch.tensor([0.0], dtype=torch.float32)\n",
    "        \n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            R = self.rewards[i] + self.gamma * R\n",
    "            returns.appendleft(R)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        returns = torch.cat(list(returns))\n",
    "        values = torch.cat(self.values)\n",
    "        log_probs = torch.cat(self.log_probs)\n",
    "        entropies = torch.cat(self.entropies)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = -(log_probs * advantages.detach()).mean() - self.entropy_coef * entropies.mean()\n",
    "        \n",
    "        # Calculate value loss\n",
    "        value_loss = nn.MSELoss()(values, returns)\n",
    "        \n",
    "        # Update networks\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        # Clip gradients to prevent large updates\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 40)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value.parameters(), 40)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Reset storage\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.entropies = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment: Simple Robotic Arm\n",
    "\n",
    "Now let's create a simple robotic environment to test our policy gradient controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRoboticArmEnv:\n",
    "    \"\"\"\n",
    "    Simplified environment for a 2-DOF robotic arm.\n",
    "    The goal is to move the end-effector to a target position.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_steps=100):\n",
    "        self.arm_length = [0.5, 0.4]  # Length of each arm segment\n",
    "        self.state_size = 4  # [joint1_angle, joint2_angle, target_x, target_y]\n",
    "        self.action_size = 2  # [change_in_joint1, change_in_joint2]\n",
    "        self.max_steps = max_steps\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Reset environment\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to a random state.\"\"\"\n",
    "        # Random starting joint angles\n",
    "        self.joint1 = np.random.uniform(-np.pi/2, np.pi/2)\n",
    "        self.joint2 = np.random.uniform(-np.pi/2, np.pi/2)\n",
    "        \n",
    "        # Random target position (within reachable range)\n",
    "        self.target_x = np.random.uniform(-0.8, 0.8)\n",
    "        self.target_y = np.random.uniform(-0.2, 0.8)  # Mostly upper half-plane\n",
    "        \n",
    "        self.step_count = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state representation.\"\"\"\n",
    "        return np.array([self.joint1, self.joint2, self.target_x, self.target_y])\n",
    "    \n",
    "    def _get_end_effector_position(self):\n",
    "        \"\"\"Calculate end-effector position based on joint angles.\"\"\"\n",
    "        # Forward kinematics for 2-DOF arm\n",
    "        x = self.arm_length[0] * np.cos(self.joint1) + \\\n",
    "            self.arm_length[1] * np.cos(self.joint1 + self.joint2)\n",
    "        y = self.arm_length[0] * np.sin(self.joint1) + \\\n",
    "            self.arm_length[1] * np.sin(self.joint1 + self.joint2)\n",
    "        return x, y\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: [delta_joint1, delta_joint2]\n",
    "            \n",
    "        Returns:\n",
    "            next_state: New state after executing action\n",
    "            reward: Scalar reward\n",
    "            done: Whether the episode is finished\n",
    "        \"\"\"\n",
    "        # Apply action (with clamping to prevent excessive movements)\n",
    "        self.joint1 += np.clip(action[0], -0.3, 0.3)\n",
    "        self.joint2 += np.clip(action[1], -0.3, 0.3)\n",
    "        \n",
    "        # Clamp joint angles to reasonable ranges\n",
    "        self.joint1 = np.clip(self.joint1, -np.pi, np.pi)\n",
    "        self.joint2 = np.clip(self.joint2, -np.pi, np.pi)\n",
    "        \n",
    "        # Get end-effector position\n",
    "        ee_x, ee_y = self._get_end_effector_position()\n",
    "        \n",
    "        # Calculate distance to target\n",
    "        dist_to_target = np.sqrt((ee_x - self.target_x)**2 + (ee_y - self.target_y)**2)\n",
    "        \n",
    "        # Reward is negative distance (higher reward = closer to target)\n",
    "        reward = -dist_to_target\n",
    "        \n",
    "        # Additional shaping reward to encourage movement toward target\n",
    "        if hasattr(self, 'prev_dist') and self.prev_dist is not None:\n",
    "            if dist_to_target < self.prev_dist:\n",
    "                reward += 0.1  # Small bonus for improvement\n",
    "            else:\n",
    "                reward -= 0.05  # Small penalty for getting farther\n",
    "        \n",
    "        self.prev_dist = dist_to_target\n",
    "        \n",
    "        # Check if target is reached\n",
    "        done = dist_to_target < 0.1 or self.step_count >= self.max_steps\n",
    "        self.step_count += 1\n",
    "        \n",
    "        return self._get_state(), reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "Now let's create the training loop for our policy gradient agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_gradient_agent(episodes=500):\n",
    "    \"\"\"\n",
    "    Train the policy gradient agent on the robotic arm environment.\n",
    "    \n",
    "    Args:\n",
    "        episodes: Number of training episodes\n",
    "        \n",
    "    Returns:\n",
    "        agent: Trained agent\n",
    "        episode_rewards: List of rewards for each episode\n",
    "    \"\"\"\n",
    "    env = SimpleRoboticArmEnv(max_steps=100)\n",
    "    agent = PolicyGradientAgent(\n",
    "        state_size=env.state_size,\n",
    "        action_size=env.action_size,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        entropy_coef=0.01\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action based on current policy\n",
    "            action, log_prob, value, entropy = agent.select_action(state)\n",
    "            \n",
    "            # Execute action in environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(log_prob, value, entropy, reward)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # Update policy after episode\n",
    "        agent.update_policy()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(env.step_count)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:]) if episode > 0 else episode_reward\n",
    "            avg_length = np.mean(episode_lengths[-50:]) if episode > 0 else env.step_count\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Average Length: {avg_length:.1f}\")\n",
    "    \n",
    "    return agent, episode_rewards, episode_lengths\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training the Policy Gradient agent...\")\n",
    "trained_agent, rewards, lengths = train_policy_gradient_agent(episodes=300)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization of Training Progress\n",
    "\n",
    "Let's visualize how the agent learned over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Episode rewards over time\n",
    "axes[0, 0].plot(rewards)\n",
    "axes[0, 0].set_title('Episode Rewards Over Time')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Smoothed rewards\n",
    "window_size = 20\n",
    "if len(rewards) > window_size:\n",
    "    smoothed_rewards = [np.mean(rewards[i:i+window_size]) \n",
    "                       for i in range(len(rewards) - window_size + 1)]\n",
    "    axes[0, 1].plot(smoothed_rewards, label='Smoothed')\n",
    "    axes[0, 1].set_title('Smoothed Episode Rewards (Window {})'.format(window_size))\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Average Reward')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].plot(rewards)\n",
    "    axes[0, 1].set_title('Episode Rewards')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Total Reward')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Episode lengths\n",
    "axes[1, 0].plot(lengths)\n",
    "axes[1, 0].set_title('Episode Lengths Over Time')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Episode Length')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Final performance histogram\n",
    "final_rewards = rewards[-50:]  # Last 50 episodes\n",
    "axes[1, 1].hist(final_rewards, bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('Distribution of Rewards (Last 50 Episodes)')\n",
    "axes[1, 1].set_xlabel('Reward')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Final 50 episodes - Average reward: {np.mean(rewards[-50:]):.2f}\")\n",
    "print(f\"Final 50 episodes - Average length: {np.mean(lengths[-50:]):.1f}\")\n",
    "print(f\"Overall average reward: {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the Trained Agent\n",
    "\n",
    "Now let's test the trained agent to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_agent(agent, episodes=5):\n",
    "    \"\"\"\n",
    "    Test the trained agent on the environment.\n",
    "    \n",
    "    Args:\n",
    "        agent: Trained PolicyGradientAgent\n",
    "        episodes: Number of test episodes\n",
    "    \"\"\"\n",
    "    env = SimpleRoboticArmEnv(max_steps=150)  # Slightly longer max steps for testing\n",
    "    \n",
    "    success_count = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        trajectory = []  # Store trajectory for visualization\n",
    "        \n",
    "        # Store initial position\n",
    "        ee_x, ee_y = env._get_end_effector_position()\n",
    "        trajectory.append((ee_x, ee_y))\n",
    "        \n",
    "        print(f\"\\nTest Episode {episode + 1}\")\n",
    "        print(f\"Target: ({env.target_x:.2f}, {env.target_y:.2f})\")\n",
    "        print(f\"Initial EE: ({ee_x:.2f}, {ee_y:.2f})\")\n",
    "        \n",
    "        while not done and steps < 150:  # Limit steps for testing\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store trajectory position\n",
    "            ee_x, ee_y = env._get_end_effector_position()\n",
    "            trajectory.append((ee_x, ee_y))\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if steps % 20 == 0:  # Print progress every 20 steps\n",
    "                dist_to_target = np.sqrt((ee_x - env.target_x)**2 + (ee_y - env.target_y)**2)\n",
    "                print(f\"  Step {steps}: EE at ({ee_x:.2f}, {ee_y:.2f}), \"\n",
    "                      f\"Dist to target: {dist_to_target:.3f}, \"\n",
    "                      f\"Reward: {reward:.2f}\")\n",
    "        \n",
    "        final_dist = np.sqrt((ee_x - env.target_x)**2 + (ee_y - env.target_y)**2)\n",
    "        success = final_dist < 0.1\n",
    "        if success:\n",
    "            success_count += 1\n",
    "        \n",
    "        print(f\"  Final: EE at ({ee_x:.2f}, {ee_y:.2f}), \"\n",
    "              f\"Final distance: {final_dist:.3f}, \"\n",
    "              f\"Total reward: {total_reward:.2f}\")\n",
    "        print(f\"  Success: {'Yes' if success else 'No'}\")\n",
    "        \n",
    "        # Plot the trajectory\n",
    "        traj_x, traj_y = zip(*trajectory)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(traj_x, traj_y, 'b-', linewidth=2, label='Robot Path')\n",
    "        plt.plot(env.target_x, env.target_y, 'go', markersize=15, label='Target')\n",
    "        plt.plot(trajectory[0][0], trajectory[0][1], 'ro', markersize=10, label='Start')\n",
    "        plt.plot(trajectory[-1][0], trajectory[-1][1], 'mo', markersize=10, label='End')\n",
    "        plt.axis('equal')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.title(f'Test Episode {episode + 1} - Success: {\"Yes\" if success else \"No\"}')\n",
    "        plt.xlabel('X Position')\n",
    "        plt.ylabel('Y Position')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    success_rate = success_count / episodes\n",
    "    print(f\"\\nOverall Success Rate: {success_rate:.1%} ({success_count}/{episodes})\")\n",
    "\n",
    "# Test the trained agent\n",
    "print(\"Testing the trained agent...\")\n",
    "test_trained_agent(trained_agent, episodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions\n",
    "\n",
    "In this notebook, we implemented and demonstrated a Policy Gradient controller for robotic systems:\n",
    "\n",
    "1. We created policy and value networks using PyTorch\n",
    "2. We implemented a Policy Gradient Agent that learns to control a robotic arm\n",
    "3. We created a simple robotic arm environment for training and testing\n",
    "4. We trained the agent and visualized the learning progress\n",
    "5. We tested the trained agent to see how well it performs\n",
    "\n",
    "Policy gradient methods are a key approach in reinforcement learning for robotics, allowing robots to learn complex behaviors through trial and error. The approach learns a direct mapping from states to actions (or action probabilities) without needing to model the environment explicitly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}